# GPT-based Language Model

This repository contains the implementation of a **Generative Pre-trained Transformer (GPT)**-based language model built using **PyTorch**. The model is trained on the text data from *The Wizard of Oz* and is capable of generating new text based on a given prompt.

## ðŸ“‹ Overview

This project showcases the process of:
- Implementing a GPT-based transformer architecture.
- Training the model on character-level data.
- Using the trained model to generate new text based on a starting prompt.

## ðŸ”§ Features

- **Self-attention mechanism**: Learn token dependencies in sequence.
- **Multi-head attention**: Multiple heads for enhanced feature learning.
- **Feedforward layers**: For dense transformation and representation learning.
- **Text generation**: Generate text from the trained language model.